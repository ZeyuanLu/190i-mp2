{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.7.12",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from typing import Dict, List, Optional\n",
        "from collections import Counter\n",
        "import os\n",
        "import csv\n",
        "!pip install torchmetrics\n",
        "!pip install pytorch-metric-learning\n",
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "!pip install pytorch-lightning\n",
        "import torch.optim as optim\n",
        "import torchmetrics\n",
        "from tqdm import tqdm\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.nn import TransformerEncoder, TransformerEncoderLayer"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-02-17T08:25:00.449032Z",
          "iopub.execute_input": "2022-02-17T08:25:00.449347Z",
          "iopub.status.idle": "2022-02-17T08:25:00.482131Z",
          "shell.execute_reply.started": "2022-02-17T08:25:00.449262Z",
          "shell.execute_reply": "2022-02-17T08:25:00.481441Z"
        },
        "trusted": true,
        "id": "SWWjBbIZ5LSi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Todo Part 1\n",
        "Complete the implementation of the encode method of the Tokenizer class:\n",
        "\n",
        "`encode`: encode a given space-separated text into list of token ids according to the `self.token2idx` property. For tokens not present in the mapping, use the id of the `<unk>` token. If `max_length` is set, pad the input to `max_length` if it is less than `max_length` and truncate to `max_length` if it exceeds the length.\n",
        "\n",
        "Examples\n",
        "```python\n",
        "text = \"hello transformers !\"\n",
        "tokenizer.encode(text)                  # example output: [3, 4, 5]\n",
        "tokenizer.encode(text, max_length=5)    # example output: [3, 4, 5, 0, 0]\n",
        "tokenizer.encode(text, max_length=2)    # example output: [3, 4]\n",
        "```"
      ],
      "metadata": {
        "id": "FF4glogI5LSk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Tokenizer:\n",
        "    def __init__(self):\n",
        "        # two special tokens for padding and unknown\n",
        "        self.token2idx = {\"<pad>\": 0, \"<unk>\": 1}\n",
        "        self.idx2token = [\"<pad>\", \"<unk>\"]\n",
        "        self.is_fit = False\n",
        "    \n",
        "    @property\n",
        "    def pad_id(self):\n",
        "        return self.token2idx[\"<pad>\"]\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.idx2token)\n",
        "    \n",
        "    def fit(self, train_texts: List[str]):\n",
        "        counter = Counter()\n",
        "        for text in train_texts:\n",
        "            counter.update(text.lower().split())\n",
        "        \n",
        "        # manually set a vocabulary size for the data set\n",
        "        vocab_size = 20000\n",
        "        self.idx2token.extend([token for token, count in counter.most_common(vocab_size - 2)])\n",
        "        for (i, token) in enumerate(self.idx2token):\n",
        "            self.token2idx[token] = i\n",
        "            \n",
        "        self.is_fit = True\n",
        "                \n",
        "    def encode(self, text: str, max_length: Optional[int] = None) -> List[int]:\n",
        "        if not self.is_fit:\n",
        "            raise Exception(\"Please fit the tokenizer on the training tokens\")\n",
        "            \n",
        "        # TODO: implement the encode method, the method signature shouldn't be changed\n",
        "        raise NotImplemented\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-02-17T08:25:00.483609Z",
          "iopub.execute_input": "2022-02-17T08:25:00.483925Z",
          "iopub.status.idle": "2022-02-17T08:25:00.494599Z",
          "shell.execute_reply.started": "2022-02-17T08:25:00.483881Z",
          "shell.execute_reply": "2022-02-17T08:25:00.493679Z"
        },
        "trusted": true,
        "id": "u29mNAdI5LSl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_raw_data(filepath: str, with_tags: bool = True):\n",
        "    data = {'text': []}\n",
        "    if with_tags:\n",
        "        data['tags'] = []\n",
        "        with open(filepath) as f:\n",
        "            reader = csv.reader(f)\n",
        "            for text, tags in reader:\n",
        "                data['text'].append(text)\n",
        "                data['tags'].append(tags)\n",
        "    else:\n",
        "        with open(filepath) as f:\n",
        "            for line in f:\n",
        "                data['text'].append(line.strip())\n",
        "    return data"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-02-17T08:25:00.495804Z",
          "iopub.execute_input": "2022-02-17T08:25:00.496201Z",
          "iopub.status.idle": "2022-02-17T08:25:00.504688Z",
          "shell.execute_reply.started": "2022-02-17T08:25:00.496166Z",
          "shell.execute_reply": "2022-02-17T08:25:00.503898Z"
        },
        "trusted": true,
        "id": "7lHbdxRn5LSm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = Tokenizer()\n",
        "train_raw = load_raw_data(os.path.join(data_dir, \"train.csv\"))\n",
        "val_raw = load_raw_data(os.path.join(data_dir, \"val.csv\"))\n",
        "test_raw = load_raw_data(os.path.join(data_dir, \"test_tokens.txt\"), with_tags=False)\n",
        "# fit the tokenizer on the training tokens\n",
        "tokenizer.fit(train_raw['text'])"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-02-17T08:25:00.517671Z",
          "iopub.execute_input": "2022-02-17T08:25:00.518378Z",
          "iopub.status.idle": "2022-02-17T08:25:00.795602Z",
          "shell.execute_reply.started": "2022-02-17T08:25:00.518340Z",
          "shell.execute_reply": "2022-02-17T08:25:00.794913Z"
        },
        "trusted": true,
        "id": "dEuoJh1Q5LSn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#upload the dataset\n",
        "#for google colb, use this\n",
        "#from google.colab import files\n",
        "#uploaded = files.upload()"
      ],
      "metadata": {
        "id": "94u1vPV-lbXf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#modify as per workspace\n",
        "tokenizer = Tokenizer()\n",
        "train_raw = load_raw_data(os.path.join(\"train.csv\"))\n",
        "val_raw = load_raw_data(os.path.join(\"val.csv\"))\n",
        "test_raw = load_raw_data(os.path.join(\"test_tokens.txt\"), with_tags=False)\n",
        "# fit the tokenizer on the training tokens\n",
        "tokenizer.fit(train_raw['text'])\n"
      ],
      "metadata": {
        "id": "jertEpiNlqXM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class NERDataset: \n",
        "    tag2idx = {'O': 1, 'B-PER': 2, 'I-PER': 3, 'B-ORG': 4, 'I-ORG': 5, 'B-LOC': 6, 'I-LOC': 7, 'B-MISC': 8, 'I-MISC': 9}\n",
        "    idx2tag = ['<pad>', 'O', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG','B-LOC', 'I-LOC', 'B-MISC', 'I-MISC']\n",
        "  \n",
        "    def __init__(self, raw_data: Dict[str, List[str]], tokenizer: Tokenizer, max_length: int = 128):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.token_ids = []\n",
        "        self.tag_ids = []\n",
        "        self.with_tags = False\n",
        "        for text in raw_data['text']:\n",
        "            self.token_ids.append(tokenizer.encode(text, max_length=max_length))\n",
        "        if 'tags' in raw_data:\n",
        "            self.with_tags = True\n",
        "            for tags in raw_data['tags']:\n",
        "                self.tag_ids.append(self.encode_tags(tags, max_length=max_length))\n",
        "    \n",
        "    def encode_tags(self, tags: str, max_length: Optional[int] = None):\n",
        "        tag_ids = [self.tag2idx[tag] for tag in tags.split()]\n",
        "        if max_length is None:\n",
        "            return tag_ids\n",
        "        # truncate the tags if longer than max_length\n",
        "        if len(tag_ids) > max_length:\n",
        "            return tag_ids[:max_length]\n",
        "        # pad with 0s if shorter than max_length\n",
        "        else:\n",
        "            return tag_ids + [0] * (max_length - len(tag_ids))  # 0 as padding for tags\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.token_ids)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        token_ids = torch.LongTensor(self.token_ids[idx])\n",
        "        mask = token_ids == self.tokenizer.pad_id  # padding tokens\n",
        "        if self.with_tags:\n",
        "            # for training and validation\n",
        "            return token_ids, mask, torch.LongTensor(self.tag_ids[idx])\n",
        "        else:\n",
        "            # for testing\n",
        "            return token_ids, mask\n",
        "        "
      ],
      "metadata": {
        "id": "KzUsGMealyZb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tr_data = NERDataset(train_raw, tokenizer)\n",
        "va_data = NERDataset(val_raw, tokenizer)\n",
        "te_data = NERDataset(test_raw, tokenizer)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-02-17T08:25:02.109558Z",
          "iopub.execute_input": "2022-02-17T08:25:02.109921Z",
          "iopub.status.idle": "2022-02-17T08:25:02.467151Z",
          "shell.execute_reply.started": "2022-02-17T08:25:02.109883Z",
          "shell.execute_reply": "2022-02-17T08:25:02.466435Z"
        },
        "trusted": true,
        "id": "0kMIKu-p5LSo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Todo Part 2\n",
        "Implement and experiment with transformer models. The implementation should include **at least** the following:\n",
        "- `nn.Embedding` layer to embed input token ids to the embedding space\n",
        "- `nn.TransformerEncoder` layer to perform transformer operations\n",
        "- `nn.Linear` layer as the output layer to map the output to the number of classes\n",
        "\n",
        "As we will be using the cross-entropy loss, an `nn.Softmax` or `nn.LogSoftmax` layer is not needed.\n",
        "\n",
        "You can refer to the following links for transformer Docs and examples:\n",
        "\n",
        "https://pytorch.org/docs/stable/_modules/torch/nn/modules/transformer.html\n",
        "\n",
        "https://pytorch.org/tutorials/beginner/transformer_tutorial.html\n",
        "\n",
        "You can modify the `__init__` method including the signature needed. For the `forward` method, the method signature is given as follows:\n",
        "\n",
        "- `src`: a `torch.LongTensor` of shape (batch_size, max_length, vocab_size) representing the input text tokens.\n",
        "\n",
        "- `src_mask`: a `torch.BoolTensor` of shape (batch_size, max_length) indicating whether an input position is padded. This is needed to prevent the transformer model attending to padded tokens.\n",
        "\n",
        "The output from the `forward` method should be of shape (batch_size, max_length, num_classes). Note that the number of classes should be 10 instead of 9 because of an additional padding class.\n"
      ],
      "metadata": {
        "id": "QVOHqRsD5LSo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: implement the Transformer model architecture and forward method\n",
        "class TransformerModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        \n",
        "    def forward(self, src: torch.Tensor, src_mask: torch.Tensor) -> torch.Tensor:\n",
        "        raise NotImplemented"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-02-17T08:25:02.468580Z",
          "iopub.execute_input": "2022-02-17T08:25:02.468821Z",
          "iopub.status.idle": "2022-02-17T08:25:02.478006Z",
          "shell.execute_reply.started": "2022-02-17T08:25:02.468786Z",
          "shell.execute_reply": "2022-02-17T08:25:02.477246Z"
        },
        "trusted": true,
        "id": "Jvy2pBQL5LSo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#modify as required\n",
        "def validate(\n",
        "    model: nn.Module, \n",
        "    dataloader: DataLoader, \n",
        "    device: torch.device,\n",
        "):\n",
        "    acc_metric = torchmetrics.Accuracy(task = 'multiclass', num_classes = 10, compute_on_step=False).to(device)\n",
        "    loss_metric = torchmetrics.MeanMetric(compute_on_step=False).to(device)\n",
        "    model.eval()\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(dataloader):\n",
        "            input_ids, input_mask, tags = batch[0].to(device), batch[1].to(device), batch[2].to(device)\n",
        "            # output shape: (batch_size, max_length, num_classes)\n",
        "            logits = model(input_ids, input_mask)\n",
        "            # ignore padding index 0 when calculating loss\n",
        "            loss = F.cross_entropy(logits.reshape(-1, 10), tags.reshape(-1), ignore_index=0)\n",
        "                \n",
        "            loss_metric.update(loss, input_mask.numel() - input_mask.sum())\n",
        "            is_active = torch.logical_not(input_mask)  # non-padding elements\n",
        "            # only consider non-padded tokens when calculating accuracy\n",
        "            acc_metric.update(logits[is_active], tags[is_active])\n",
        "    \n",
        "    print(f\"| Validate | loss {loss_metric.compute():.4f} | acc {acc_metric.compute():.4f} |\")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-02-17T08:25:03.082423Z",
          "iopub.execute_input": "2022-02-17T08:25:03.082694Z",
          "iopub.status.idle": "2022-02-17T08:25:03.091306Z",
          "shell.execute_reply.started": "2022-02-17T08:25:03.082660Z",
          "shell.execute_reply": "2022-02-17T08:25:03.090373Z"
        },
        "trusted": true,
        "id": "Y5Eaibzu5LSp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#modify as required\n",
        "def train(\n",
        "    model: nn.Module, \n",
        "    dataloader: DataLoader, \n",
        "    optimizer: optim.Optimizer,\n",
        "    device: torch.device,\n",
        "    epoch: int,\n",
        "):\n",
        "    acc_metric = torchmetrics.Accuracy(task = 'multiclass', num_classes = 10, compute_on_step=False).to(device)\n",
        "    loss_metric = torchmetrics.MeanMetric(compute_on_step=False).to(device)\n",
        "    model.train()\n",
        "    \n",
        "    # loop through all batches in the training\n",
        "    for batch in tqdm(dataloader):\n",
        "        input_ids, input_mask, tags = batch[0].to(device), batch[1].to(device), batch[2].to(device)\n",
        "        optimizer.zero_grad()\n",
        "        # output shape: (batch_size, max_length, num_classes)\n",
        "        logits = model(input_ids, input_mask)\n",
        "        # ignore padding index 0 when calculating loss\n",
        "        loss = F.cross_entropy(logits.reshape(-1, 10), tags.reshape(-1), ignore_index=0)\n",
        "        \n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        loss_metric.update(loss, input_mask.numel() - input_mask.sum())\n",
        "        is_active = torch.logical_not(input_mask)  # non-padding elements\n",
        "        # only consider non-padded tokens when calculating accuracy\n",
        "        acc_metric.update(logits[is_active], tags[is_active])\n",
        "    \n",
        "    print(f\"| Epoch {epoch} | loss {loss_metric.compute():.4f} | acc {acc_metric.compute():.4f} |\")\n",
        "    "
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-02-17T08:25:03.092754Z",
          "iopub.execute_input": "2022-02-17T08:25:03.093232Z",
          "iopub.status.idle": "2022-02-17T08:25:03.104319Z",
          "shell.execute_reply.started": "2022-02-17T08:25:03.093195Z",
          "shell.execute_reply": "2022-02-17T08:25:03.103543Z"
        },
        "trusted": true,
        "id": "qQtTOXRA5LSp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#modify as required\n",
        "torch.manual_seed(42)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# data loaders\n",
        "train_dataloader = DataLoader(tr_data, batch_size=32, shuffle=True)\n",
        "val_dataloader = DataLoader(va_data, batch_size=32)\n",
        "test_dataloader = DataLoader(te_data, batch_size=32)\n",
        "\n",
        "# move the model to device\n",
        "model = TransformerModel(vocab_size = len(tokenizer), \n",
        "    embed_size = 256, \n",
        "    num_heads = 4, \n",
        "    hidden_size = 256,\n",
        "    num_layers = 2,).to(device)\n",
        "\n",
        "optimizer = optim.Adam(model.parameters())\n",
        "\n",
        "for epoch in range(5):\n",
        "    train(model, train_dataloader, optimizer, device, epoch)\n",
        "validate(model, val_dataloader, device)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-02-17T08:25:03.105602Z",
          "iopub.execute_input": "2022-02-17T08:25:03.105982Z",
          "iopub.status.idle": "2022-02-17T08:25:43.205981Z",
          "shell.execute_reply.started": "2022-02-17T08:25:03.105946Z",
          "shell.execute_reply": "2022-02-17T08:25:43.205084Z"
        },
        "trusted": true,
        "id": "5Be4ZCs15LSq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Todo Part 3\n",
        "Make predictions on the validation data and evaluate entity-level F1 scores using conlleval script.\n",
        "\n",
        "`predict`: taking inputs of a trained model, a dataloader, and a torch device, predict the tags for all tokens in the data set. The output should be a nested list of lists, each containing tag predictions for a single sentence.\n",
        "\n",
        "    Input texts in the dataloader (2 sentences):\n",
        "    EU rejects German call\n",
        "    Only France and Britain backed Fischler 's proposal .\n",
        "    \n",
        "    Example output:\n",
        "    [['B-ORG', 'O', 'B-MISC', 'O'], ['O', 'B-LOC', 'O', 'B-LOC', 'O', 'B-PER', 'O', 'O', 'O']]\n",
        "        "
      ],
      "metadata": {
        "id": "uQV7JhRl5LSq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: implement the predict function\n",
        "def predict(model: nn.Module, dataloader: DataLoader, device: torch.device) -> List[List[str]]:\n",
        "    model.eval()\n",
        "    preds = []\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(dataloader):\n",
        "            raise NotImplemented\n",
        "                    \n",
        "    return preds"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-02-17T08:25:43.207551Z",
          "iopub.execute_input": "2022-02-17T08:25:43.207812Z",
          "iopub.status.idle": "2022-02-17T08:25:43.426052Z",
          "shell.execute_reply.started": "2022-02-17T08:25:43.207774Z",
          "shell.execute_reply": "2022-02-17T08:25:43.425331Z"
        },
        "trusted": true,
        "id": "2BeTuu4i5LSq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://raw.githubusercontent.com/sighsmile/conlleval/master/conlleval.py\n",
        "from conlleval import evaluate"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-02-17T08:25:43.430210Z",
          "iopub.execute_input": "2022-02-17T08:25:43.431127Z",
          "iopub.status.idle": "2022-02-17T08:25:44.519771Z",
          "shell.execute_reply.started": "2022-02-17T08:25:43.431084Z",
          "shell.execute_reply": "2022-02-17T08:25:44.519005Z"
        },
        "trusted": true,
        "id": "EzFjEe0c5LSq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# use the conlleval script to measure the entity-level f1\n",
        "pred_tags = []\n",
        "for tags in predict(model, val_dataloader, device):\n",
        "    pred_tags.extend(tags)\n",
        "    pred_tags.append('O')\n",
        "    \n",
        "true_tags = []\n",
        "for tags in val_raw['tags']:\n",
        "    true_tags.extend(tags.strip().split())\n",
        "    true_tags.append('O')\n",
        "\n",
        "evaluate(true_tags, pred_tags)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-02-17T08:25:44.521224Z",
          "iopub.execute_input": "2022-02-17T08:25:44.521511Z",
          "iopub.status.idle": "2022-02-17T08:25:45.394177Z",
          "shell.execute_reply.started": "2022-02-17T08:25:44.521470Z",
          "shell.execute_reply": "2022-02-17T08:25:45.393518Z"
        },
        "trusted": true,
        "id": "lVAnQYdD5LSr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Example output from the above codeblock. We will take the overall test F1 score (69.24 in this example) and grade accordingly.\n",
        "```\n",
        "processed 54612 tokens with 5942 phrases; found: 5554 phrases; correct: 3980.\n",
        "accuracy:  65.78%; (non-O)\n",
        "accuracy:  93.88%; precision:  71.66%; recall:  66.98%; FB1:  69.24\n",
        "              LOC: precision:  84.58%; recall:  77.03%; FB1:  80.63  1673\n",
        "             MISC: precision:  77.31%; recall:  71.69%; FB1:  74.40  855\n",
        "              ORG: precision:  58.71%; recall:  63.83%; FB1:  61.16  1458\n",
        "              PER: precision:  66.84%; recall:  56.89%; FB1:  61.47  1568\n",
        "(71.66006481814908, 66.98081454055873, 69.24147529575504)\n",
        "```\n",
        "If the codeblock above errors out, check your implementation of the `predict` function. It should return a nested list of lists, each containing predicted tags in their IOB string forms."
      ],
      "metadata": {
        "id": "ztKqd9J15LSr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Todo Part 4\n",
        "Once you finish all previous todos and are satisfied with the model performance on the validation set, make predictions on the test set and keep a copy of the `submission.txt` file by downloading it to your local machine. You can find `submission.txt` under Output > `/kaggle/working`."
      ],
      "metadata": {
        "id": "IaChyXkY5LSr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# YOU SHOULD NOT CHANGE THIS CODEBLOCK\n",
        "# make prediction on the test set and save to submission.txt\n",
        "preds = predict(model, test_dataloader, device)\n",
        "with open(\"submission.txt\", \"w\") as f:\n",
        "    for tags in preds:\n",
        "        f.write(\" \".join(tags) + \"\\n\")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-02-17T08:25:45.395550Z",
          "iopub.execute_input": "2022-02-17T08:25:45.395822Z",
          "iopub.status.idle": "2022-02-17T08:25:46.111937Z",
          "shell.execute_reply.started": "2022-02-17T08:25:45.395787Z",
          "shell.execute_reply": "2022-02-17T08:25:46.111234Z"
        },
        "trusted": true,
        "id": "dVt102qy5LSs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pwd"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-02-17T08:27:24.950886Z",
          "iopub.execute_input": "2022-02-17T08:27:24.951166Z",
          "iopub.status.idle": "2022-02-17T08:27:24.957143Z",
          "shell.execute_reply.started": "2022-02-17T08:27:24.951135Z",
          "shell.execute_reply": "2022-02-17T08:27:24.956414Z"
        },
        "trusted": true,
        "id": "jJOtOKyR5LSs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ls"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-02-17T08:27:30.109566Z",
          "iopub.execute_input": "2022-02-17T08:27:30.109841Z",
          "iopub.status.idle": "2022-02-17T08:27:30.796831Z",
          "shell.execute_reply.started": "2022-02-17T08:27:30.109811Z",
          "shell.execute_reply": "2022-02-17T08:27:30.795875Z"
        },
        "trusted": true,
        "id": "5VquJfjq5LSs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ioUQaKri5LSs"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}